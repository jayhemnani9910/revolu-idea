# CAG Deep Research System - Environment Configuration
# Copy this file to `.env` and fill in your API keys.

# === LLM (API) Configuration ===

# Option 1: GitHub Models (FREE with Copilot subscription)
# Get your token: https://github.com/settings/tokens (with "models:read" scope)
# Rate limits: 15 RPM / 150 RPD for Low tier, 10 RPM / 50 RPD for High tier
LLM_PROVIDER=github
LLM_BASE_URL=https://models.inference.ai.azure.com
LLM_MODEL=gpt-4o-mini
LLM_API_KEY=your-github-token-here

# Option 2: DeepSeek (OpenAI-compatible)
# LLM_PROVIDER=deepseek
# LLM_BASE_URL=https://api.deepseek.com
# LLM_MODEL=deepseek-chat
# LLM_API_KEY=your-api-key-here

# Option 3: Groq (OpenAI-compatible endpoint)
# LLM_PROVIDER=groq
# LLM_BASE_URL=https://api.groq.com/openai/v1
# Use "auto" to use the built-in Groq model pool (guard/safeguard models are skipped),
# or provide a comma-separated list to control it.
# LLM_MODEL=auto
# LLM_API_KEY=your-groq-key-here

# === Ollama (local) Configuration (kept, but not used by default) ===
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen3:8b

# Generation parameters
TEMPERATURE=0.0
MAX_TOKENS=4096

# === Search Configuration ===
# Free option (no key): DuckDuckGo (may be rate-limited / less stable).
SEARCH_PROVIDER=duckduckgo

# Paid/high-quality options:
# SEARCH_PROVIDER=tavily
# TAVILY_API_KEY=tvly-your-tavily-key-here
# SEARCH_PROVIDER=exa
# EXA_API_KEY=your-exa-key-here

# === Research Parameters ===
MAX_RECURSION_DEPTH=5
MAX_INVESTIGATIONS_PER_EDGE=2

# === Storage Configuration ===
OUTPUT_DIR=output
